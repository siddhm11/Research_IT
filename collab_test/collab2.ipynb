{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340c0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qdrant-client in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (1.14.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: InstructorEmbedding in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from qdrant-client) (1.73.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from qdrant-client) (6.31.1)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from qdrant-client) (2.11.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from qdrant-client) (2.5.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (308)\n",
      "Requirement already satisfied: filelock in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sidha\\anaconda3\\envs\\researchit\\lib\\site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install qdrant-client datasets InstructorEmbedding pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"Qdrant/arxiv-titles-instructorxl-embeddings\", split=\"train\", streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04d8c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iterator = iter(dataset)\n",
    "train_dataset = [next(dataset_iterator) for _ in range(60000)]\n",
    "test_dataset = [next(dataset_iterator) for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a400d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"XXXXXXXXXXXXXXXXXXXXXXXXXX\", \n",
    "    api_key=\"XXXXXXXXXXXXXXXXXXXXXXXX\",\n",
    "    timeout=120.0,\n",
    ")\n",
    "\n",
    "COLL_DENSE = \"arxiv-titles-dense\"\n",
    "\n",
    "# Create once; skip if it already exists\n",
    "if not client.collection_exists(COLL_DENSE):\n",
    "    client.create_collection(\n",
    "        collection_name=COLL_DENSE,\n",
    "        vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed70ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import PointStruct\n",
    "import uuid, itertools\n",
    "\n",
    "def batched(iterable, n=50):          # simple chunker\n",
    "    it = iter(iterable)\n",
    "    while (chunk := list(itertools.islice(it, n))):\n",
    "        yield chunk\n",
    "\n",
    "points = (\n",
    "    PointStruct(\n",
    "        id=str(uuid.uuid4()),\n",
    "        vector=item[\"vector\"],            # 768-element list\n",
    "        payload={\"title\": item[\"title\"], \"paper_id\": i}\n",
    "    )\n",
    "    for i, item in enumerate(train_dataset)\n",
    ")\n",
    "\n",
    "for batch in batched(points, 100):\n",
    "    client.upsert(collection_name=COLL_DENSE, points=batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8093ebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidha\\AppData\\Local\\Temp\\ipykernel_53672\\3541450983.py:8: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(COLL_DENSE, query_vector=vec, limit=k, with_payload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Quantum Error Correction via Noise Guessing Decoding', 0.8623227), ('Error correction in ensemble registers for quantum repeaters and quantum\\n  computers', 0.8551971), ('Quantum data processing and error correction', 0.8479152), ('Introduction to Quantum Error Correction', 0.84624475), ('Introduction to Quantum Error Correction', 0.84624475), ('Quantum Error Correction', 0.84499454), ('Quantum Error Correction', 0.84499454), ('Quantum Error Correction of a Qubit Loss in an Addressable Atomic System', 0.8444803), ('Quantum f-divergences and error correction', 0.8439431), ('Quantum f-divergences and error correction', 0.8439431)]\n"
     ]
    }
   ],
   "source": [
    "def search_by_title(title:str, k=10):\n",
    "    # embed the query title locally (use the same Instructor-XL model)\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    ENCODER_PATH = \"models/instructor-xl\"\n",
    "    model = SentenceTransformer(ENCODER_PATH, device=\"cpu\")\n",
    "    vec = model.encode([[\"Represent the Science title:\", title]])[0]\n",
    "\n",
    "    hits = client.search(COLL_DENSE, query_vector=vec, limit=k, with_payload=True)\n",
    "    return [(h.payload[\"title\"], h.score) for h in hits]\n",
    "\n",
    "print(search_by_title(\"Quantum error correction in noisy qubits\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab65058d",
   "metadata": {},
   "source": [
    "### 2.1 Generate or import user ratings\n",
    "If you have real clicks/ratings, load them into a pandas DataFrame with columns user_id paper_id rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f35640b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "NUM_USERS = 2000\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "rows = []\n",
    "for u in range(NUM_USERS):\n",
    "    n_items = rng.integers(20, 120)        # each user rates 20-120 papers\n",
    "    items   = rng.choice(len(train_dataset), n_items, replace=False)\n",
    "    ratings = rng.normal(0, 1, n_items)    # mean-0 std-1 (already normalized)\n",
    "    rows.extend(zip([u]*n_items, items, ratings))\n",
    "\n",
    "ratings_df = pd.DataFrame(rows, columns=[\"user_id\",\"paper_id\",\"rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c030c",
   "metadata": {},
   "source": [
    "#### 2.2 Convert each user to a sparse vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29ba228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "user_sparse = defaultdict(lambda: {\"indices\":[], \"values\":[]})\n",
    "\n",
    "for r in ratings_df.itertuples():\n",
    "    user_sparse[r.user_id][\"indices\"].append(int(r.paper_id))\n",
    "    user_sparse[r.user_id][\"values\"].append(float(r.rating))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6055130",
   "metadata": {},
   "source": [
    "#### 2.3 Store sparse vectors in Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bb14216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import SparseVectorParams, SparseVector, PointStruct\n",
    "\n",
    "COLL_SPARSE = \"user-paper-sparse\"\n",
    "if not client.collection_exists(COLL_SPARSE):\n",
    "    client.create_collection(\n",
    "        collection_name=COLL_SPARSE,\n",
    "        vectors_config={},        # no dense vectors here\n",
    "        sparse_vectors_config={\"ratings\": SparseVectorParams()}   # default index in-RAM\n",
    "    )\n",
    "\n",
    "def gen_points():\n",
    "    for uid, vec in user_sparse.items():\n",
    "        yield PointStruct(\n",
    "            id=uid,\n",
    "            vector={\"ratings\": SparseVector(indices=vec[\"indices\"], values=vec[\"values\"])},\n",
    "            payload={\"user_id\": uid, \"rated\": vec[\"indices\"]}\n",
    "        )\n",
    "\n",
    "client.upload_points(COLL_SPARSE, points=gen_points())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a921779a",
   "metadata": {},
   "source": [
    "#### 2.4 Collaborative-filter query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9bed1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_cf(my_ratings:dict, k_users=20):\n",
    "    idxs, vals = zip(*my_ratings.items())\n",
    "    query_vec  = SparseVector(indices=list(idxs), values=list(vals))\n",
    "\n",
    "    users = client.query_points(\n",
    "        collection_name=COLL_SPARSE,\n",
    "        query=query_vec,\n",
    "        using=\"ratings\",\n",
    "        limit=k_users\n",
    "    ).points\n",
    "\n",
    "    # gather scores for unseen items\n",
    "    from collections import Counter\n",
    "    scores = Counter()\n",
    "    for u in users:\n",
    "        for pid in u.payload[\"rated\"]:\n",
    "            if pid not in my_ratings:\n",
    "                scores[pid] += u.score\n",
    "    return scores.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60659b",
   "metadata": {},
   "source": [
    "#### 1. Content-Based Search (Dense Vectors)\n",
    "This finds papers similar to a given paper or query text using the title embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4812dd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidha\\AppData\\Local\\Temp\\ipykernel_53672\\3644330590.py:9: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NIST: An Image Classification Network to Image Semantic Retrieval', 0.8323139), ('NIST: An Image Classification Network to Image Semantic Retrieval', 0.8323139), ('Combined convolutional and recurrent neural networks for hierarchical\\n  classification of images', 0.8287244), ('Design of Kernels in Convolutional Neural Networks for Image\\n  Classification', 0.82021785), ('Design of Kernels in Convolutional Neural Networks for Image\\n  Classification', 0.82021785), ('Pollen Grain Microscopic Image Classification Using an Ensemble of\\n  Fine-Tuned Deep Convolutional Neural Networks', 0.81991637), ('Pollen Grain Microscopic Image Classification Using an Ensemble of\\n  Fine-Tuned Deep Convolutional Neural Networks', 0.81991637), ('RAIN: A Simple Approach for Robust and Accurate Image Classification\\n  Networks', 0.8181366), ('RAIN: A Simple Approach for Robust and Accurate Image Classification\\n  Networks', 0.8181366), ('Compressive spectral image classification using 3D coded convolutional\\n  neural network', 0.8112027)]\n"
     ]
    }
   ],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def search_by_title(title, k=10):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    ENCODER_PATH = \"models/instructor-xl\"\n",
    "    model = SentenceTransformer(ENCODER_PATH, device=\"cpu\")\n",
    "    vec = model.encode([[\"Represent the Science title:\", title]])[0]  # Get dense embedding\n",
    "    hits = client.search(\n",
    "        collection_name=COLL_DENSE,\n",
    "        query_vector=vec,\n",
    "        limit=k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return [(hit.payload[\"title\"], hit.score) for hit in hits]\n",
    "\n",
    "results = search_by_title(\"Neural networks for image classification\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51edb192",
   "metadata": {},
   "source": [
    "#### 2. Collaborative Filtering Search (Sparse Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aca2897a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7033, -0.28070956), (24221, -0.28070956), (25612, -0.28070956), (41940, -0.28070956), (4969, -0.28070956), (32550, -0.28070956), (5603, -0.28070956), (1598, -0.28070956), (33134, -0.28070956), (52587, -0.28070956)]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.models import SparseVector\n",
    "\n",
    "def recommend_cf(my_ratings, k_users=20):\n",
    "    indices, values = zip(*my_ratings.items())\n",
    "    query_vec = SparseVector(indices=list(indices), values=list(values))\n",
    "    users = client.query_points(\n",
    "        collection_name=COLL_SPARSE,\n",
    "        query=query_vec,\n",
    "        using=\"ratings\",\n",
    "        limit=k_users\n",
    "    ).points\n",
    "\n",
    "    from collections import Counter\n",
    "    scores = Counter()\n",
    "    for user in users:\n",
    "        for pid in user.payload[\"rated\"]:\n",
    "            if pid not in my_ratings:\n",
    "                scores[pid] += user.score\n",
    "    return scores.most_common(10)\n",
    "\n",
    "# Example: rate papers 123 (like) and 456 (dislike)\n",
    "my_ratings = {123: 1.0, 456: -1.0}\n",
    "recommended_papers = recommend_cf(my_ratings)\n",
    "print(recommended_papers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d5bdf",
   "metadata": {},
   "source": [
    "#### Step 1: Generate and Normalize Synthetic Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29ba254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "NUM_USERS = 100\n",
    "NUM_PAPERS = len(train_dataset)\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "rows = []\n",
    "for u in range(NUM_USERS):\n",
    "    n_items = rng.integers(20, 80)\n",
    "    items = rng.choice(NUM_PAPERS, n_items, replace=False)\n",
    "    ratings = rng.normal(0, 1, n_items)  # mean 0, std 1\n",
    "    rows.extend(zip([u]*n_items, items, ratings))\n",
    "\n",
    "ratings_df = pd.DataFrame(rows, columns=[\"user_id\", \"paper_id\", \"rating\"])\n",
    "# (No need to further normalize, you already have mean 0, std 1 random ratings!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a7767",
   "metadata": {},
   "source": [
    "#### Step 2: Build User Sparse Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "138afc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "user_sparse = defaultdict(lambda: {\"indices\": [], \"values\": []})\n",
    "for row in ratings_df.itertuples():\n",
    "    user_sparse[row.user_id][\"indices\"].append(int(row.paper_id))\n",
    "    user_sparse[row.user_id][\"values\"].append(float(row.rating))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f8a791",
   "metadata": {},
   "source": [
    "#### Step 3: Create Qdrant Sparse Collection and Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9241447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import SparseVectorParams, SparseVector, PointStruct\n",
    "\n",
    "COLL_SPARSE = \"user-paper-sparse\"\n",
    "if not client.collection_exists(COLL_SPARSE):\n",
    "    client.create_collection(\n",
    "        collection_name=COLL_SPARSE,\n",
    "        vectors_config={},  # no dense vectors\n",
    "        sparse_vectors_config={\"ratings\": SparseVectorParams()}\n",
    "    )\n",
    "\n",
    "def gen_points():\n",
    "    for uid, vec in user_sparse.items():\n",
    "        yield PointStruct(\n",
    "            id=uid,\n",
    "            vector={\"ratings\": SparseVector(indices=vec[\"indices\"], values=vec[\"values\"])},\n",
    "            payload={\"user_id\": uid, \"rated\": vec[\"indices\"]}\n",
    "        )\n",
    "\n",
    "client.upload_points(COLL_SPARSE, points=gen_points())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682798d",
   "metadata": {},
   "source": [
    "#### Step 4: Implement the Collaborative Filtering Query Function\n",
    "Goal: For a demo, pick a realistic user and use their ratings as the query so you’re guaranteed overlap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98393fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import SparseVector\n",
    "\n",
    "def recommend_cf(my_ratings, k_users=10, top_n=10):\n",
    "    indices, values = zip(*my_ratings.items())\n",
    "    query_vec = SparseVector(indices=list(indices), values=list(values))\n",
    "    users = client.query_points(\n",
    "        collection_name=COLL_SPARSE,\n",
    "        query=query_vec,\n",
    "        using=\"ratings\",\n",
    "        limit=k_users\n",
    "    ).points\n",
    "\n",
    "    # Aggregate unseen paper recommendations\n",
    "    from collections import Counter\n",
    "    scores = Counter()\n",
    "    for user in users:\n",
    "        for pid in user.payload[\"rated\"]:\n",
    "            if pid not in my_ratings:\n",
    "                scores[pid] += user.score\n",
    "    return scores.most_common(top_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23f77a3",
   "metadata": {},
   "source": [
    "#### Step 5: Demo—Pick Example Ratings from Existing Data\n",
    "Goal: Pick an actual user and use their ratings for guaranteed overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d616c20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo query ratings: {43728: 0.6836861907765345, 41122: 1.0039615758421696, 40220: -0.6179070447076008, 1698: 1.8220113633283233, 51404: -1.3204309700132935}\n",
      "Paper IDs and scores: [(34731, 6.9204383), (38173, 6.9204383), (10504, 6.9204383), (990, 6.9204383), (51757, 6.9204383), (50726, 6.9204383), (48285, 6.9204383), (28847, 6.9204383), (5353, 6.9204383), (45860, 6.9204383)]\n"
     ]
    }
   ],
   "source": [
    "example_user = ratings_df['user_id'].iloc[0]\n",
    "example_ratings = dict(zip(\n",
    "    ratings_df[ratings_df['user_id'] == example_user]['paper_id'],\n",
    "    ratings_df[ratings_df['user_id'] == example_user]['rating']\n",
    "))\n",
    "\n",
    "# For clarity, you may want to only use a subset (e.g., 5 ratings) in your query:\n",
    "my_ratings_demo = dict(list(example_ratings.items())[:5])\n",
    "print(\"Demo query ratings:\", my_ratings_demo)\n",
    "\n",
    "recommendations = recommend_cf(my_ratings_demo)\n",
    "print(\"Paper IDs and scores:\", recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c51b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResearchIT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
