{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RRF-Enhanced Paper Recommendation Feed with Decay Learning\n\n## Architecture\n- **4 User Embeddings**: Complete, Subject1, Subject2, Subject3\n- **RRF Fusion**: Fuse rankings from semantic query, complete profile, subject-focused, and recency\n- **MMR Diversification**: Apply Maximal Marginal Relevance for balanced recommendations\n- **Decay Learning**: Time-weighted interaction updates to vectors\n- **Feed API**: Paginated personalized feed with interaction logging","metadata":{}},{"cell_type":"code","source":"# Install dependencies\n!pip install -q feedparser\n!pip uninstall -y xformers fastai easyocr timm 2>/dev/null\n!pip install --extra-index-url https://download.pytorch.org/whl/cpu torch==2.9.0+cpu torchvision==0.24.0+cpu torchaudio==2.9.0+cpu\n!pip install transformers==4.53.0 --upgrade\n!pip install -q sentence-transformers qdrant-client numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T13:02:34.145026Z","iopub.execute_input":"2025-11-03T13:02:34.152785Z","iopub.status.idle":"2025-11-03T13:02:56.965776Z","shell.execute_reply.started":"2025-11-03T13:02:34.152659Z","shell.execute_reply":"2025-11-03T13:02:56.963649Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\nRequirement already satisfied: torch==2.9.0+cpu in /usr/local/lib/python3.11/dist-packages (2.9.0+cpu)\nRequirement already satisfied: torchvision==0.24.0+cpu in /usr/local/lib/python3.11/dist-packages (0.24.0+cpu)\nRequirement already satisfied: torchaudio==2.9.0+cpu in /usr/local/lib/python3.11/dist-packages (2.9.0+cpu)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.9.0+cpu) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.0+cpu) (4.15.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.0+cpu) (1.14.0)\nRequirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.0+cpu) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.0+cpu) (3.1.6)\nRequirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.0+cpu) (2025.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.24.0+cpu) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.24.0+cpu) (11.3.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.9.0+cpu) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.9.0+cpu) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.24.0+cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.24.0+cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.24.0+cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.24.0+cpu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.24.0+cpu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.24.0+cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.24.0+cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.24.0+cpu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.24.0+cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.24.0+cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.24.0+cpu) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers==4.53.0 in /usr/local/lib/python3.11/dist-packages (4.53.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0) (3.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.53.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.53.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.53.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.53.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.53.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.53.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.53.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.53.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.53.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.53.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.53.0) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import os\nimport math\nimport json\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom typing import List, Optional, Dict, Tuple\nfrom enum import Enum\nimport numpy as np\n\n# Qdrant credentials\nQDRANT_URL = \"https://553c68d4-edc5-4369-aef3-83ac014d1682.eu-central-1-0.aws.cloud.qdrant.io\"\nQDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.yB3LJ-xh91hA4h-LKuX4XPpYt8Dp7BXDOyeykt6G2Zc\"\nCOLLECTION_NAME = \"arxiv_stella_1024_recommendations\"\n\n# Model config\nMODEL_NAME = \"NovaSearch/stella_en_400M_v5\"\nEMBEDDING_DIM = 1024\n\n# Learning parameters from userfeed.py\nALPHA = 0.10  # Learning rate\nDECAY_RATE = 0.1  # Exponential decay\nMIN_WEIGHT_THRESHOLD = 0.05\nMAX_INTERACTION_AGE_DAYS = 365\n\n# Interaction weights\nBASE_WEIGHTS = {\"LIKE\": 1.0, \"BOOKMARK\": 0.8, \"VIEW\": 0.3, \"DISLIKE\": -0.5}\n\n# RRF parameters\nRRF_K = 60  # Standard RRF constant\nRRF_RETRIEVER_LIMIT = 100  # Results per retriever\n\nprint(\"‚úì Configuration loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:12:24.563042Z","iopub.execute_input":"2025-11-03T14:12:24.570238Z","iopub.status.idle":"2025-11-03T14:12:24.605436Z","shell.execute_reply.started":"2025-11-03T14:12:24.570130Z","shell.execute_reply":"2025-11-03T14:12:24.603394Z"}},"outputs":[{"name":"stdout","text":"‚úì Configuration loaded\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# Enum definitions\nclass VectorType(Enum):\n    COMPLETE = \"complete\"\n    SUBJECT1 = \"subject1\"\n    SUBJECT2 = \"subject2\"\n    SUBJECT3 = \"subject3\"\n\nclass InteractionType(Enum):\n    LIKE = \"like\"\n    DISLIKE = \"dislike\"\n    VIEW = \"view\"\n    BOOKMARK = \"bookmark\"\n\n@dataclass\nclass Interaction:\n    \"\"\"User interaction with temporal decay\"\"\"\n    arxiv_id: str\n    interaction_type: InteractionType\n    timestamp: datetime\n    subject_area: Optional[str] = None\n    base_weight: float = field(init=False)\n    \n    def __post_init__(self):\n        self.base_weight = BASE_WEIGHTS.get(self.interaction_type.value.upper(), 0.3)\n    \n    def age_days(self) -> float:\n        return (datetime.now() - self.timestamp).total_seconds() / 86400\n    \n    def get_decayed_weight(self) -> float:\n        age = self.age_days()\n        if age > MAX_INTERACTION_AGE_DAYS:\n            return 0.0\n        decayed = self.base_weight * math.exp(-DECAY_RATE * age)\n        return 0.0 if abs(decayed) < MIN_WEIGHT_THRESHOLD else decayed\n\n@dataclass\nclass MMRResult:\n    \"\"\"Result from MMR ranking\"\"\"\n    arxiv_id: str\n    relevance_score: float\n    diversity_score: float\n    mmr_score: float\n    rank: int\n    payload: Dict = field(default_factory=dict)\n\n@dataclass\nclass RRFResult:\n    \"\"\"Result from RRF fusion\"\"\"\n    arxiv_id: str\n    rrf_score: float\n    vector: Optional[np.ndarray] = None\n    payload: Dict = field(default_factory=dict)\n    retriever_ranks: Dict[str, int] = field(default_factory=dict)  # Track rank from each retriever\n\n@dataclass\nclass FeedItem:\n    \"\"\"Item in personalized feed\"\"\"\n    arxiv_id: str\n    title: str\n    authors: List[str]\n    abstract: str\n    rrf_score: float\n    mmr_score: float\n    relevance_score: float\n    diversity_score: float\n    rank: int\n    pdf_url: str\n    abs_url: str\n    published: str\n    categories: List[str] = field(default_factory=list)\n\nprint(\"‚úì Data structures defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:12:25.591638Z","iopub.execute_input":"2025-11-03T14:12:25.592042Z","iopub.status.idle":"2025-11-03T14:12:25.630603Z","shell.execute_reply.started":"2025-11-03T14:12:25.592016Z","shell.execute_reply":"2025-11-03T14:12:25.629315Z"}},"outputs":[{"name":"stdout","text":"‚úì Data structures defined\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom qdrant_client import models\n\nclient = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, timeout=60)\nmodel = SentenceTransformer(\n    MODEL_NAME,\n    trust_remote_code=True,\n    device=\"cpu\",\n    config_kwargs={\"use_memory_efficient_attention\": False, \"unpad_inputs\": False}\n)\n\nprint(\"‚úì Clients initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:12:27.868909Z","iopub.execute_input":"2025-11-03T14:12:27.869815Z","iopub.status.idle":"2025-11-03T14:12:30.078782Z","shell.execute_reply.started":"2025-11-03T14:12:27.869782Z","shell.execute_reply":"2025-11-03T14:12:30.077615Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at NovaSearch/stella_en_400M_v5 were not used when initializing NewModel: ['pooler.dense.bias', 'pooler.dense.weight']\n- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"‚úì Clients initialized\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"# Core utility functions\n\ndef l2_norm(x: np.ndarray) -> np.ndarray:\n    n = np.linalg.norm(x)\n    return x if n == 0 else x / n\n\ndef encode_text(text: str, is_query: bool = True) -> np.ndarray:\n    \"\"\"Encode text using Stella model\"\"\"\n    v = model.encode(\n        text,\n        prompt_name=\"s2p_query\" if is_query else \"s2p_passage\",\n        convert_to_numpy=True\n    ).astype(np.float32)\n    return l2_norm(v)\n\ndef cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"Compute cosine similarity\"\"\"\n    return float(np.dot(a, b))\n\ndef retrieve_vector_by_arxiv_id(arxiv_id: str) -> Optional[np.ndarray]:\n    \"\"\"Retrieve paper vector from Qdrant\"\"\"\n    flt = models.Filter(\n        must=[\n            models.FieldCondition(\n                key=\"arxiv_id\",\n                match=models.MatchValue(value=str(arxiv_id))\n            )\n        ]\n    )\n    resp = client.query_points(\n        collection_name=COLLECTION_NAME,\n        query_filter=flt,\n        with_payload=True,\n        with_vectors=True,\n        limit=1\n    )\n    pts = getattr(resp, \"points\", []) or []\n    if not pts:\n        return None\n    return l2_norm(np.array(pts[0].vector, dtype=np.float32))\n\nprint(\"‚úì Helper functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:12:31.711227Z","iopub.execute_input":"2025-11-03T14:12:31.711564Z","iopub.status.idle":"2025-11-03T14:12:31.724842Z","shell.execute_reply.started":"2025-11-03T14:12:31.711542Z","shell.execute_reply":"2025-11-03T14:12:31.723316Z"}},"outputs":[{"name":"stdout","text":"‚úì Helper functions defined\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"# arXiv API integration\nimport time\nfrom functools import lru_cache\nimport urllib.parse\nimport feedparser\n\nARXIV_API = \"http://export.arxiv.org/api/query\"\n\ndef normalize_arxiv_id(aid: str) -> str:\n    aid = (aid or \"\").strip()\n    if aid.lower().startswith(\"arxiv:\"):\n        aid = aid[6:]\n    return aid\n\ndef parse_entry(entry):\n    \"\"\"Parse arXiv API entry\"\"\"\n    fullid = entry.id.rsplit(\"/\", 1)[-1]\n    baseid = fullid.split(\"v\")[0]\n    \n    title = entry.title.strip()\n    summary = getattr(entry, \"summary\", \"\").strip()\n    published = getattr(entry, \"published\", None)\n    updated = getattr(entry, \"updated\", None)\n    authors = [a.name for a in getattr(entry, \"authors\", [])]\n    \n    absurl, pdfurl, doiurl = None, None, None\n    for link in getattr(entry, \"links\", []):\n        rel = getattr(link, \"rel\", \"\")\n        titleattr = getattr(link, \"title\", \"\")\n        href = getattr(link, \"href\", \"\")\n        if rel == \"alternate\" and \"abs\" in href:\n            absurl = href\n        if titleattr == \"pdf\" or (rel == \"related\" and getattr(link, \"type\", \"\") == \"application/pdf\"):\n            pdfurl = href\n        if titleattr == \"doi\":\n            doiurl = href\n    \n    categories = [t.get(\"term\") for t in getattr(entry, \"tags\", []) if isinstance(t, dict) and t.get(\"term\")]\n    primarycat = entry.arxiv_primary_category.get(\"term\") if hasattr(entry, \"arxiv_primary_category\") else None\n    comment = getattr(entry, \"arxiv_comment\", None)\n    journalref = getattr(entry, \"arxiv_journal_ref\", None)\n    doi = getattr(entry, \"arxiv_doi\", None) or doiurl\n    \n    return {\n        \"arxiv_id_full\": fullid,\n        \"arxiv_id\": baseid,\n        \"title\": title,\n        \"summary\": summary,\n        \"authors\": authors,\n        \"published\": published,\n        \"updated\": updated,\n        \"categories\": categories,\n        \"primary_category\": primarycat,\n        \"comment\": comment,\n        \"journal_ref\": journalref,\n        \"doi\": doi,\n        \"absurl\": absurl or f\"https://arxiv.org/abs/{fullid}\",\n        \"pdfurl\": pdfurl or f\"https://arxiv.org/pdf/{fullid}.pdf\",\n    }\n\ndef fetch_batch(ids_chunk):\n    params = {\"id_list\": \",\".join(ids_chunk)}\n    url = f\"{ARXIV_API}?{urllib.parse.urlencode(params)}\"\n    feed = feedparser.parse(url)\n    results = {}\n    for e in getattr(feed, \"entries\", []):\n        if getattr(e, \"title\", \"\").strip().lower() == \"error\":\n            continue\n        r = parse_entry(e)\n        results[r[\"arxiv_id\"]] = r\n    return results\n\ndef fetch_arxiv_by_ids(ids, chunksize=100, sleep_between=0.0):\n    normed = [normalize_arxiv_id(x) for x in ids if x]\n    out = {}\n    for i in range(0, len(normed), chunksize):\n        chunk = normed[i:i+chunksize]\n        out.update(fetch_batch(chunk))\n        if sleep_between > 0:\n            time.sleep(sleep_between)\n    return out\n\n@lru_cache(maxsize=8192)\ndef fetch_arxiv_one(arxiv_id: str):\n    res = fetch_arxiv_by_ids([arxiv_id])\n    return res.get(normalize_arxiv_id(arxiv_id))\n\nprint(\"‚úì arXiv API client ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:12:33.483205Z","iopub.execute_input":"2025-11-03T14:12:33.483627Z","iopub.status.idle":"2025-11-03T14:12:33.503893Z","shell.execute_reply.started":"2025-11-03T14:12:33.483602Z","shell.execute_reply":"2025-11-03T14:12:33.502661Z"}},"outputs":[{"name":"stdout","text":"‚úì arXiv API client ready\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"\n\nclass UserProfile:\n    \"\"\"User profile with 4 embeddings, decay learning, and RRF support\"\"\"\n    \n    def __init__(self, userid: str):\n        self.userid = userid\n        self.embedding_dim = EMBEDDING_DIM\n        \n        # Initialize 4 user vectors\n        self.vectors = {\n            VectorType.COMPLETE: np.zeros(EMBEDDING_DIM, dtype=np.float32),\n            VectorType.SUBJECT1: np.zeros(EMBEDDING_DIM, dtype=np.float32),\n            VectorType.SUBJECT2: np.zeros(EMBEDDING_DIM, dtype=np.float32),\n            VectorType.SUBJECT3: np.zeros(EMBEDDING_DIM, dtype=np.float32),\n        }\n        \n        # Subject metadata\n        self.subjects = {\n            VectorType.SUBJECT1: {\"name\": \"Large Language Models\", \"keywords\": \"llm, gpt, transformer\"},\n            VectorType.SUBJECT2: {\"name\": \"Reinforcement Learning\", \"keywords\": \"rl, policy, reward\"},\n            VectorType.SUBJECT3: {\"name\": \"Computer Vision\", \"keywords\": \"image, vision, cnn\"},\n        }\n        \n        # Interaction history\n        self.interactions: List[Interaction] = []\n        \n        # MMR parameters\n        self.mmr_lambda = 0.7  # 70% relevance, 30% diversity\n        \n        print(f\"‚úì Created user profile {userid}\")\n    \n    def onboard_from_topics(self, topic_weights: Dict[str, float], k_per_topic: int = 50):\n        \"\"\"Initialize user vectors from topic interests\"\"\"\n        print(f\"Onboarding user with topics {topic_weights}\")\n        \n        total = sum(max(0, w) for w in topic_weights.values())\n        if total == 0:\n            raise ValueError(\"topic_weights must have positive mass\")\n        norm_weights = {t: max(0, w) / total for t, w in topic_weights.items()}\n        \n        centroids = {}\n        for topic, weight in norm_weights.items():\n            if weight == 0:\n                continue\n            print(f\"  Searching {topic}...\")\n            q = encode_text(topic)\n            \n            results = client.query_points(\n                collection_name=COLLECTION_NAME,\n                query=q.tolist(),\n                limit=k_per_topic,\n                with_vectors=True\n            )\n            \n            vecs = [l2_norm(np.array(r.vector, dtype=np.float32)) for r in results.points if r.vector]\n            if vecs:\n                centroids[topic] = l2_norm(np.mean(np.stack(vecs), axis=0))\n        \n        # Weighted combination for complete vector\n        complete_vec = np.zeros(EMBEDDING_DIM, dtype=np.float32)\n        for topic, weight in norm_weights.items():\n            if topic in centroids:\n                complete_vec += weight * centroids[topic]\n        self.vectors[VectorType.COMPLETE] = l2_norm(complete_vec)\n        \n        # Assign centroids to subject slots\n        topic_list = list(centroids.keys())\n        for i, vtype in enumerate([VectorType.SUBJECT1, VectorType.SUBJECT2, VectorType.SUBJECT3]):\n            if i < len(topic_list):\n                self.vectors[vtype] = centroids[topic_list[i]]\n                self.subjects[vtype][\"name\"] = topic_list[i]\n        \n        print(\"‚úì Onboarding complete!\")\n\n    def add_interaction(self, arxiv_id: str, interaction_type: str, timestamp: datetime = None):\n        \"\"\"Record interaction and update vectors via decay learning\"\"\"\n        if timestamp is None:\n            timestamp = datetime.now()\n        \n        interaction = Interaction(\n            arxiv_id=arxiv_id,\n            interaction_type=InteractionType[interaction_type.upper()],\n            timestamp=timestamp\n        )\n        self.interactions.append(interaction)\n        \n        # Get paper vector\n        paper_vec = retrieve_vector_by_arxiv_id(arxiv_id)\n        if paper_vec is None:\n            print(f\"Paper {arxiv_id} not found, skipping update\")\n            return\n        \n        # Apply decay learning to complete vector\n        weight = interaction.get_decayed_weight()\n        if weight != 0:\n            self.vectors[VectorType.COMPLETE] = l2_norm(\n                self.vectors[VectorType.COMPLETE] + ALPHA * weight * paper_vec\n            )\n\n        \n        subject_types = [VectorType.SUBJECT1, VectorType.SUBJECT2, VectorType.SUBJECT3]\n        \n        # 1. Calculate positive similarities\n        sims = {vtype: cosine_similarity(paper_vec, self.vectors[vtype]) for vtype in subject_types}\n        pos_sims = {vtype: sim for vtype, sim in sims.items() if sim > 0}\n\n        if pos_sims:\n            # 2. Normalize positive similarities (so they sum to 1)\n            total_sim = sum(pos_sims.values())\n            norm_sims = {vtype: (sim / total_sim) for vtype, sim in pos_sims.items()}\n            \n            # 3. Apply weighted updates to all relevant subjects\n            for vtype, norm_weight in norm_sims.items():\n                if weight != 0:\n                    # Update is gated by base weight (LIKE/DISLIKE) and proportional weight\n                    proportional_update = ALPHA * weight * norm_weight * paper_vec\n                    self.vectors[vtype] = l2_norm(\n                        self.vectors[vtype] + proportional_update\n                    )\n            \n            print(f\"‚úì Proportional update for {interaction_type} on {arxiv_id} across {len(pos_sims)} subjects.\")\n        \n        else:\n            print(f\"‚úì No subject vectors were relevant for {interaction_type} on {arxiv_id}.\")\n\n\n    def get_personalized_query(self, original_query: str, \n                             complete_weight: float = 0.5,\n                             subject_weight: float = 0.3) -> np.ndarray:\n        \"\"\"Blend user preferences with query\"\"\"\n        query_vec = l2_norm(encode_text(original_query))\n        \n        # Find most relevant subject\n        subject_vec = None\n        best_sim = -1\n        for vtype in [VectorType.SUBJECT1, VectorType.SUBJECT2, VectorType.SUBJECT3]:\n            sim = cosine_similarity(query_vec, self.vectors[vtype])\n            if sim > best_sim:\n                best_sim = sim\n                subject_vec = self.vectors[vtype]\n        \n        if subject_vec is None:\n            subject_vec = np.zeros(EMBEDDING_DIM, dtype=np.float32)\n        \n        # Weighted blend\n        base_weight = 1.0 - complete_weight - subject_weight\n        personalized = (\n            base_weight * query_vec\n            + complete_weight * self.vectors[VectorType.COMPLETE]\n            + subject_weight * subject_vec\n        )\n        return l2_norm(personalized)\n    \n    def apply_mmr_ranking(self, search_results: List[Dict], query_vec: np.ndarray,\n                        lambda_param: float = None, max_results: int = 20) -> List[MMRResult]:\n        \"\"\"Apply MMR for diversity\"\"\"\n        if lambda_param is None:\n            lambda_param = self.mmr_lambda\n        \n        if not search_results:\n            return []\n        \n        result_vecs = []\n        result_meta = []\n        for r in search_results:\n            if \"vector\" in r:\n                result_vecs.append(r[\"vector\"])\n                result_meta.append(r)\n        \n        if not result_vecs:\n            return []\n        \n        result_vecs = np.array(result_vecs)\n        \n        # MMR algorithm\n        selected = []\n        remaining = list(range(len(result_vecs)))\n        \n        for rank in range(min(max_results, len(result_vecs))):\n            best_score = -float('inf')\n            best_idx = None\n            \n            for idx in remaining:\n                relevance = np.dot(result_vecs[idx], query_vec)\n                \n                if selected:\n                    selected_vecs = result_vecs[[r.rank for r in selected]]\n                    max_sim = np.max(np.dot(selected_vecs, result_vecs[idx]))\n                    diversity = 1 - max_sim\n                else:\n                    diversity = 0\n                \n                mmr_score = lambda_param * relevance + (1 - lambda_param) * diversity\n                if mmr_score > best_score:\n                    best_score = mmr_score\n                    best_idx = idx\n            \n            if best_idx is not None:\n                relevance = float(np.dot(result_vecs[best_idx], query_vec))\n                diversity = 1 - max_sim if selected else 0\n                \n                selected.append(MMRResult(\n                    arxiv_id=result_meta[best_idx].get(\"arxiv_id\", f\"paper_{best_idx}\"),\n                    relevance_score=relevance,\n                    diversity_score=diversity,\n                    mmr_score=float(best_score),\n                    rank=best_idx,\n                    payload=result_meta[best_idx]\n                ))\n                remaining.remove(best_idx)\n        \n        return selected\n\nprint(\"‚úì UserProfile class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:12:36.203607Z","iopub.execute_input":"2025-11-03T14:12:36.203908Z","iopub.status.idle":"2025-11-03T14:12:36.237159Z","shell.execute_reply.started":"2025-11-03T14:12:36.203888Z","shell.execute_reply":"2025-11-03T14:12:36.236063Z"}},"outputs":[{"name":"stdout","text":"‚úì UserProfile class defined\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"# RRF (Reciprocal Rank Fusion) Implementation\n\ndef rrf_fuse(ranked_lists: List[List[Dict]], k: int = RRF_K) -> List[RRFResult]:\n    \"\"\"\n    Fuse multiple ranked lists using Reciprocal Rank Fusion.\n    \n    RRF Score = Œ£ (1 / (k + rank_i))\n    \n    Args:\n        ranked_lists: List of ranked paper lists from different retrievers\n        k: RRF constant (default 60)\n    \n    Returns:\n        Sorted list of fused results by RRF score\n    \"\"\"\n    rrf = {}\n    pos_maps = []\n    \n    # --- MODIFICATION ---\n    # Dynamically get retriever names from the input\n    retriever_names = [r[\"name\"] for r in ranked_lists]\n    \n    # Build position maps for each retriever\n    for list_idx, retriever in enumerate(ranked_lists):\n        lst = retriever[\"list\"]\n        pos = {item[\"arxiv_id\"]: (i + 1) for i, item in enumerate(lst)}\n        pos_maps.append(pos)\n    \n    # Get union of all arxiv_ids\n    keys = set()\n    for pos in pos_maps:\n        keys.update(pos.keys())\n    \n    # Compute RRF scores\n    for aid in keys:\n        score = 0.0\n        retriever_ranks = {}\n        for list_idx, pos in enumerate(pos_maps):\n            if aid in pos:\n                rank = pos[aid]\n                score += 1.0 / (k + rank)\n                retriever_ranks[retriever_names[list_idx]] = rank\n        rrf[aid] = (score, retriever_ranks)\n    \n    # Build fused list with metadata cache\n    cache = {}\n    for retriever in ranked_lists:\n        for item in retriever[\"list\"]:\n            cache.setdefault(item[\"arxiv_id\"], item)\n    \n    fused = []\n    for aid, (score, retriever_ranks) in sorted(rrf.items(), key=lambda x: x[1][0], reverse=True):\n        item = dict(cache.get(aid, {\"arxiv_id\": aid}))\n        fused.append(RRFResult(\n            arxiv_id=aid,\n            rrf_score=score,\n            vector=item.get(\"vector\"),\n            payload=item.get(\"payload\", {}),\n            retriever_ranks=retriever_ranks\n        ))\n    \n    return fused\n    \n#\n# üõë REPLACE ALL OF CELL 10 WITH THIS CODE üõë\n#\n\ndef build_retrievers(user: UserProfile, query_vec: np.ndarray, has_query_text: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Build ranked lists from 5-6 complementary retrievers for RRF.\n\n    Retrievers:\n    1. Semantic Query: (OPTIONAL) Personalized query + user preferences\n    2. Complete Profile: (ALWAYS) Pure user profile vector\n    3. Subject 1: (ALWAYS) Vector for subject 1\n    4. Subject 2: (ALWAYS) Vector for subject 2\n    5. Subject 3: (ALWAYS) Vector for subject 3\n    6. Recency: (ALWAYS) Recently published papers from all retrievers\n    \"\"\"\n    retrievers = {}\n\n    # 1. Semantic Query Retriever (CONDITIONAL)\n    list_sem_query = []\n    if has_query_text:\n        print(\"Building semantic query retriever...\")\n        raw_results = client.query_points(\n            collection_name=COLLECTION_NAME,\n            query=query_vec.tolist(),\n            limit=RRF_RETRIEVER_LIMIT,\n            with_vectors=True,\n            with_payload=True\n        )\n        for i, r in enumerate(raw_results.points):\n            list_sem_query.append({\n                \"arxiv_id\": r.payload.get(\"arxiv_id\", f\"unknown_{i}\"),\n                \"vector\": l2_norm(np.array(r.vector, dtype=np.float32)),\n                \"payload\": r.payload,\n                \"rank\": i + 1,\n                \"raw_score\": r.score\n            })\n    retrievers[\"semantic_query\"] = list_sem_query\n    print(f\"  ‚úì Found {len(list_sem_query)} results (Semantic Query)\")\n\n    # 2. Complete Profile Retriever (ALWAYS RUNS)\n    print(\"Building complete profile retriever...\")\n    raw_results_complete = client.query_points(\n        collection_name=COLLECTION_NAME,\n        query=user.vectors[VectorType.COMPLETE].tolist(),\n        limit=RRF_RETRIEVER_LIMIT,\n        with_vectors=True,\n        with_payload=True\n    )\n    list_complete = []\n    for i, r in enumerate(raw_results_complete.points):\n        list_complete.append({\n            \"arxiv_id\": r.payload.get(\"arxiv_id\", f\"unknown_{i}\"),\n            \"vector\": l2_norm(np.array(r.vector, dtype=np.float32)),\n            \"payload\": r.payload,\n            \"rank\": i + 1,\n            \"raw_score\": r.score\n        })\n    retrievers[\"complete_profile\"] = list_complete\n    print(f\"  ‚úì Found {len(list_complete)} results (Complete Profile)\")\n\n    # ------------------------------------------------------------------\n    # ‚¨áÔ∏è START OF MAJOR CODE CHANGE (Replacing \"Subject-Focused\")\n    # ------------------------------------------------------------------\n\n    # 3. Subject 1 Retriever (ALWAYS RUNS)\n    print(\"Building Subject 1 retriever...\")\n    raw_results_s1 = client.query_points(\n        collection_name=COLLECTION_NAME,\n        query=user.vectors[VectorType.SUBJECT1].tolist(),\n        limit=RRF_RETRIEVER_LIMIT,\n        with_vectors=True,\n        with_payload=True\n    )\n    list_subject1 = []\n    for i, r in enumerate(raw_results_s1.points):\n        list_subject1.append({\n            \"arxiv_id\": r.payload.get(\"arxiv_id\", f\"unknown_{i}\"),\n            \"vector\": l2_norm(np.array(r.vector, dtype=np.float32)),\n            \"payload\": r.payload,\n            \"rank\": i + 1,\n            \"raw_score\": r.score\n        })\n    retrievers[\"subject1\"] = list_subject1\n    print(f\"  ‚úì Found {len(list_subject1)} results ({user.subjects[VectorType.SUBJECT1]['name']})\")\n\n\n    # 4. Subject 2 Retriever (ALWAYS RUNS)\n    print(\"Building Subject 2 retriever...\")\n    raw_results_s2 = client.query_points(\n        collection_name=COLLECTION_NAME,\n        query=user.vectors[VectorType.SUBJECT2].tolist(),\n        limit=RRF_RETRIEVER_LIMIT,\n        with_vectors=True,\n        with_payload=True\n    )\n    list_subject2 = []\n    for i, r in enumerate(raw_results_s2.points):\n        list_subject2.append({\n            \"arxiv_id\": r.payload.get(\"arxiv_id\", f\"unknown_{i}\"),\n            \"vector\": l2_norm(np.array(r.vector, dtype=np.float32)),\n            \"payload\": r.payload,\n            \"rank\": i + 1,\n            \"raw_score\": r.score\n        })\n    retrievers[\"subject2\"] = list_subject2\n    print(f\"  ‚úì Found {len(list_subject2)} results ({user.subjects[VectorType.SUBJECT2]['name']})\")\n\n\n    # 5. Subject 3 Retriever (ALWAYS RUNS)\n    print(\"Building Subject 3 retriever...\")\n    raw_results_s3 = client.query_points(\n        collection_name=COLLECTION_NAME,\n        query=user.vectors[VectorType.SUBJECT3].tolist(),\n        limit=RRF_RETRIEVER_LIMIT,\n        with_vectors=True,\n        with_payload=True\n    )\n    list_subject3 = []\n    for i, r in enumerate(raw_results_s3.points):\n        list_subject3.append({\n            \"arxiv_id\": r.payload.get(\"arxiv_id\", f\"unknown_{i}\"),\n            \"vector\": l2_norm(np.array(r.vector, dtype=np.float32)),\n            \"payload\": r.payload,\n            \"rank\": i + 1,\n            \"raw_score\": r.score\n        })\n    retrievers[\"subject3\"] = list_subject3\n    print(f\"  ‚úì Found {len(list_subject3)} results ({user.subjects[VectorType.SUBJECT3]['name']})\")\n\n\n\n    print(\"Building Discovery retriever (serendipity)...\")\n    \n    # 1. Create a \"perturbed\" vector by adding small random noise\n    # This \"nudges\" the search into adjacent vector space\n    noise = np.random.normal(0, 0.05, EMBEDDING_DIM).astype(np.float32)\n    discovery_vec = l2_norm(user.vectors[VectorType.COMPLETE] + noise)\n\n    # 2. Query with this new discovery vector\n    raw_results_disc = client.query_points(\n        collection_name=COLLECTION_NAME,\n        query=discovery_vec.tolist(),\n        limit=RRF_RETRIEVER_LIMIT,\n        with_vectors=True,\n        with_payload=True\n    )\n    list_discovery = []\n    for i, r in enumerate(raw_results_disc.points):\n        list_discovery.append({\n            \"arxiv_id\": r.payload.get(\"arxiv_id\", f\"unknown_{i}\"),\n            \"vector\": l2_norm(np.array(r.vector, dtype=np.float32)),\n            \"payload\": r.payload,\n            \"rank\": i + 1,\n            \"raw_score\": r.score\n        })\n    retrievers[\"discovery\"] = list_discovery\n    print(f\"  ‚úì Found {len(list_discovery)} results (Discovery)\")\n\n    \n    # ------------------------------------------------------------------\n    # ‚¨ÜÔ∏è END OF MAJOR CODE CHANGE\n    # ------------------------------------------------------------------\n\n    # 6. Recency Retriever (ALWAYS RUNS)\n    print(\"Building recency retriever...\")\n\n    # --- THIS IS THE KEY CHANGE for RECENCY---\n    # The union must now include all 5 lists (semantic, complete, s1, s2, s3)\n    union_ids = list(set(\n        [item[\"arxiv_id\"] for item in list_sem_query + list_complete + list_subject1 + list_subject2 + list_subject3 + list_discovery\n         if \"unknown\" not in item[\"arxiv_id\"]]\n    ))\n\n    \n    if union_ids:\n        meta = fetch_arxiv_by_ids(union_ids, sleep_between=0.1)\n        recency_sorted = sorted(\n            meta.keys(),\n            key=lambda aid: (meta[aid].get(\"updated\") or meta[aid].get(\"published\") or \"\"),\n            reverse=True\n        )\n        list_recency = []\n        for i, aid in enumerate(recency_sorted):\n            # Get vector if available from cache\n            vec = None\n            # (Update this loop to check all lists)\n            for item in list_sem_query + list_complete + list_subject1 + list_subject2 + list_subject3 + list_discovery:\n                if item[\"arxiv_id\"] == aid:\n                    vec = item[\"vector\"]\n                    break\n            list_recency.append({\n                \"arxiv_id\": aid,\n                \"vector\": vec,\n                \"payload\": {\"arxiv_meta\": meta[aid]},\n                \"rank\": i + 1,\n                \"raw_score\": 1.0 / (i + 1)\n            })\n        retrievers[\"recency\"] = list_recency\n        print(f\"  ‚úì Found {len(list_recency)} unique papers (Recency)\")\n    else:\n        retrievers[\"recency\"] = []\n        print(\"  ‚úì No papers for recency ranking\")\n\n    return retrievers\n\nprint(\"‚úì RRF and retriever functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:12:40.589908Z","iopub.execute_input":"2025-11-03T14:12:40.590275Z","iopub.status.idle":"2025-11-03T14:12:40.626356Z","shell.execute_reply.started":"2025-11-03T14:12:40.590250Z","shell.execute_reply":"2025-11-03T14:12:40.624989Z"}},"outputs":[{"name":"stdout","text":"‚úì RRF and retriever functions defined\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"#\n# üõë REPLACE ALL OF CELL 70 WITH THIS CODE üõë\n#\n\nclass FeedService:\n    \"\"\"Personalized feed service with RRF + MMR + decay learning\"\"\"\n\n    def __init__(self, user: UserProfile):\n        self.user = user\n        self.feed_impressions = {}  # Track impressions for analytics\n\n    \n    def _get_cold_start_feed(self, page: int = 1, page_size: int = 10) -> List[FeedItem]:\n        \"\"\"\n        Generates a non-personalized feed for new users.\n        Bypasses RRF and serves popular/recent generic items.\n        \"\"\"\n        print(\"  - Executing Cold Start: Serving generic 'LLM' feed.\")\n        try:\n            # Query for a generic, high-quality term\n            query_vec = encode_text(\"Large Language Models and transformers\", is_query=True)\n            \n            raw_results = client.query_points(\n                collection_name=COLLECTION_NAME,\n                query=query_vec.tolist(),\n                limit=page_size * page, # Get enough for pagination\n                with_vectors=True,\n                with_payload=True\n            )\n            \n            # Paginate\n            start = (page - 1) * page_size\n            end = start + page_size\n            page_results = raw_results.points[start:end]\n            \n            # Enrich and return (simplified enrichment)\n            arxiv_ids = [r.payload.get(\"arxiv_id\") for r in page_results if r.payload]\n            meta = fetch_arxiv_by_ids(arxiv_ids, sleep_between=0.05) if arxiv_ids else {}\n            \n            feed_items = []\n            for rank, result in enumerate(page_results, start=1):\n                aid = result.payload.get(\"arxiv_id\", \"unknown\")\n                m = meta.get(aid, {})\n                item = FeedItem(\n                    arxiv_id=aid,\n                    title=m.get(\"title\", result.payload.get(\"title\", \"Unknown Title\")),\n                    authors=m.get(\"authors\", [])[:5],\n                    abstract=m.get(\"summary\", \"\")[:200],\n                    rrf_score=0.0,\n                    mmr_score=result.score, # Use raw score as MMR score\n                    relevance_score=result.score,\n                    diversity_score=0.0,\n                    rank=rank,\n                    pdf_url=m.get(\"pdfurl\", \"\"),\n                    abs_url=m.get(\"absurl\", \"\"),\n                    published=m.get(\"published\", \"\"),\n                    categories=m.get(\"categories\", [])\n                )\n                feed_items.append(item)\n            \n            print(f\"‚úÖ Cold Start feed generated with {len(feed_items)} items\\n\")\n            return feed_items\n\n        except Exception as e:\n            print(f\"  - ‚ö†Ô∏è Cold Start Error: {e}\")\n            return [] # Return empty list on failure\n    \n\n    \n    def get_feed(self, query: Optional[str] = None, page: int = 1, page_size: int = 20,\n                 apply_mmr: bool = True, mmr_lambda: float = 0.7) -> List[FeedItem]:\n        \"\"\"\n        Generate personalized feed using RRF + MMR pipeline.\n        \n        Pipeline:\n        1. Determine Mode: \"Search\" (if query) or \"Feed\" (if no query)\n        2. Set Base Vector: Blended query OR pure profile vector\n        3. Build 5-6 retrievers (semantic, complete, s1, s2, s3, recency)\n        4. Fuse rankings with RRF\n        5. Apply MMR for diversity\n        6. Filter seen items\n        7. Paginate and enrich with metadata\n        \"\"\"\n        \n        print(f\"\\nüìù Generating feed for page {page}...\")\n\n        # --- NEW \"A+\" COLD START CHECK ---\n        has_query_text = (query is not None) and (query.strip() != \"\")\n        is_cold_start = np.linalg.norm(self.user.vectors[VectorType.COMPLETE]) < 0.01\n\n        if is_cold_start and not has_query_text:\n            print(\"  - Mode: Cold Start (No profile, no query)\")\n            # Call the new helper function and exit early\n            return self._get_cold_start_feed(page, page_size)\n            \n        # Step 1: Determine Mode and Set Base Query Vector\n        # has_query_text = (query is not None) and (query.strip() != \"\") # <--- This was redundant, I removed it\n\n        if has_query_text:\n            print(\"  - Mode: Personalized Search (Query + Profile)\")\n            query_vec = self.user.get_personalized_query(query, 0.5, 0.3)\n        else:\n            print(\"  - Mode: Recommendation Feed (Profile Only)\")\n            query_vec = self.user.vectors[VectorType.COMPLETE]\n\n        # Step 2: Build retrievers\n        print(\"\\n1Ô∏è‚É£ Building retrievers for RRF fusion...\")\n        retrievers = build_retrievers(self.user, query_vec, has_query_text)\n\n        # Step 3: RRF fusion\n        print(\"\\n2Ô∏è‚É£ Applying RRF fusion...\")\n        \n        ranked_lists = []\n        if retrievers[\"semantic_query\"]:\n            ranked_lists.append({\"name\": \"semantic_query\", \"list\": retrievers[\"semantic_query\"]})\n            print(\"  - Fusing: Semantic Query list\")\n        if retrievers[\"complete_profile\"]:\n            ranked_lists.append({\"name\": \"complete_profile\", \"list\": retrievers[\"complete_profile\"]})\n            print(\"  - Fusing: Complete Profile list\")\n        if retrievers[\"subject1\"]:\n            ranked_lists.append({\"name\": \"subject1\", \"list\": retrievers[\"subject1\"]})\n            print(\"  - Fusing: Subject 1 list\")\n        if retrievers[\"subject2\"]:\n            ranked_lists.append({\"name\": \"subject2\", \"list\": retrievers[\"subject2\"]})\n            print(\"  - Fusing: Subject 2 list\")\n        if retrievers[\"subject3\"]:\n            ranked_lists.append({\"name\": \"subject3\", \"list\": retrievers[\"subject3\"]})\n            print(\"  - Fusing: Subject 3 list\")\n        if retrievers[\"discovery\"]:\n            ranked_lists.append({\"name\": \"discovery\", \"list\": retrievers[\"discovery\"]})\n            print(\"  - Fusing: Discovery list\")\n        if retrievers[\"recency\"]:\n            ranked_lists.append({\"name\": \"recency\", \"list\": retrievers[\"recency\"]})\n            print(\"  - Fusing: Recency list\")\n\n        fused = rrf_fuse(ranked_lists, k=RRF_K)\n        print(f\"  ‚úì RRF fused {len(fused)} papers from {len(ranked_lists)} lists\")\n\n        # Step 4: Backfill vectors and apply MMR\n        print(\"\\n3Ô∏è‚É£ Preparing for MMR ranking...\")\n        search_results = []\n        for item in fused[:min(200, len(fused))]: \n            if item.vector is None:\n                item.vector = retrieve_vector_by_arxiv_id(item.arxiv_id)\n            if item.vector is not None:\n                search_results.append({\n                    \"arxiv_id\": item.arxiv_id,\n                    \"vector\": item.vector,\n                    \"payload\": item.payload,\n                    \"rrf_score\": item.rrf_score,\n                    \"retriever_ranks\": item.retriever_ranks\n                })\n\n        # Step 5: Apply MMR if requested\n        if apply_mmr:\n            print(f\"\\n4Ô∏è‚É£ Applying MMR diversity filter (top {len(search_results)} by RRF)...\")\n            buffer_size = 50 # Fetch a larger buffer to account for filtering\n            mmr_results = self.user.apply_mmr_ranking(\n                search_results,\n                query_vec,\n                lambda_param=mmr_lambda,\n                max_results=buffer_size \n            )\n        else:\n            mmr_results = [MMRResult(\n                arxiv_id=sr[\"arxiv_id\"],\n                relevance_score=sr.get(\"rrf_score\", 0.0),\n                diversity_score=0.0,\n                mmr_score=sr.get(\"rrf_score\", 0.0),\n                rank=idx,\n                payload=sr[\"payload\"]\n            ) for idx, sr in enumerate(search_results[:50])] # Apply buffer size here too\n        \n        print(f\"  ‚úì MMR selected {len(mmr_results)} papers for buffer.\")\n\n        # Step 6: Filter interacted items (from the larger buffer)\n        interacted_ids_to_hide = set(\n            interaction.arxiv_id for interaction in self.user.interactions\n            if interaction.interaction_type in (InteractionType.LIKE, InteractionType.BOOKMARK, InteractionType.DISLIKE)\n        )\n        \n        filtered_results = [r for r in mmr_results if r.arxiv_id not in interacted_ids_to_hide]\n        print(f\"  ‚úì Filtered out {len(mmr_results) - len(filtered_results)} already-interacted items from buffer.\")\n\n        # Step 7: Paginate (from the *filtered* list)\n        start = (page - 1) * page_size\n        end = start + page_size\n        page_results = filtered_results[start:end] \n\n        # Step 8: Enrich with metadata\n        print(f\"\\n5Ô∏è‚É£ Enriching {len(page_results)} items with metadata...\")\n        arxiv_ids = [r.arxiv_id for r in page_results]\n        \n        # --- THIS IS THE MISSING LINE THAT CAUSED THE NameError ---\n        meta = fetch_arxiv_by_ids(arxiv_ids, sleep_between=0.05) if arxiv_ids else {}\n        # ---------------------------------------------------------\n\n        feed_items = []\n        for rank, result in enumerate(page_results, start=1):\n            m = meta.get(result.arxiv_id, {})\n            item = FeedItem(\n                arxiv_id=result.arxiv_id,\n                title=m.get(\"title\", \"Unknown Title\"),\n                authors=m.get(\"authors\", [])[:5],\n                abstract=m.get(\"summary\", \"\")[:200],\n                rrf_score=result.payload.get(\"rrf_score\", 0.0),\n                mmr_score=result.mmr_score,\n                relevance_score=result.relevance_score,\n                diversity_score=result.diversity_score,\n                rank=rank,\n                pdf_url=m.get(\"pdfurl\", \"\"),\n                abs_url=m.get(\"absurl\", \"\"),\n                published=m.get(\"published\", \"\"),\n                categories=m.get(\"categories\", [])\n            )\n            feed_items.append(item)\n\n        # --- THIS BLOCK IS NOW REDUNDANT AND HAS BEEN REMOVED ---\n        # interacted_ids_to_hide = set(...)\n        # final_feed = [item for item in feed_items ...]\n        # print(f\"  ‚úì Filtered out ...\")\n        # --------------------------------------------------------\n        \n        print(f\"‚úÖ Feed generated with {len(feed_items)} items\\n\")\n        return feed_items # This list is now correctly filtered AND paginated\n\n    def log_interaction(self, arxiv_id: str, interaction_type: str):\n        \"\"\"Log user interaction and update profile\"\"\"\n        self.user.add_interaction(arxiv_id, interaction_type)\n        if arxiv_id not in self.feed_impressions:\n            self.feed_impressions[arxiv_id] = {}\n        self.feed_impressions[arxiv_id][interaction_type] = datetime.now()\n        print(f\"‚úì Logged {interaction_type} on {arxiv_id}\")\n\nprint(\"‚úì FeedService class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:19:32.808832Z","iopub.execute_input":"2025-11-03T14:19:32.809292Z","iopub.status.idle":"2025-11-03T14:19:32.840924Z","shell.execute_reply.started":"2025-11-03T14:19:32.809260Z","shell.execute_reply":"2025-11-03T14:19:32.839722Z"}},"outputs":[{"name":"stdout","text":"‚úì FeedService class defined\n","output_type":"stream"}],"execution_count":74},{"cell_type":"markdown","source":"## Example: Initialize User & Generate Initial Feed","metadata":{}},{"cell_type":"code","source":"# Create user and onboard from topics\nprint(\"üöÄ Initializing user profile...\\n\")\nuser = UserProfile(\"demo_user_001\")\n\n# Onboard with interests\ntopic_weights = {\n    \"Large Language Models \": 0.35,\n    \"Reinforcement Learning\": 0.3,\n    \"Economics with Machine learning\": 0.35\n}\n\nuser.onboard_from_topics(topic_weights, k_per_topic=50)\n\n# Display initialized vectors\nprint(\"\\nüìä User Vector Status:\")\nfor vtype, vec in user.vectors.items():\n    norm = np.linalg.norm(vec)\n    print(f\"  {vtype.value:15s} | norm: {norm:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:20:46.279207Z","iopub.execute_input":"2025-11-03T14:20:46.279633Z","iopub.status.idle":"2025-11-03T14:20:49.495645Z","shell.execute_reply.started":"2025-11-03T14:20:46.279606Z","shell.execute_reply":"2025-11-03T14:20:49.494499Z"}},"outputs":[{"name":"stdout","text":"üöÄ Initializing user profile...\n\n‚úì Created user profile demo_user_001\nOnboarding user with topics {'Large Language Models ': 0.35, 'Reinforcement Learning': 0.3, 'Economics with Machine learning': 0.35}\n  Searching Large Language Models ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24b714d911c848b3a9567c2fd3093d5a"}},"metadata":{}},{"name":"stdout","text":"  Searching Reinforcement Learning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0844ca0984c642d2a03226324be34816"}},"metadata":{}},{"name":"stdout","text":"  Searching Economics with Machine learning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe9ef496a39440f0a67f3241af6cfdb4"}},"metadata":{}},{"name":"stdout","text":"‚úì Onboarding complete!\n\nüìä User Vector Status:\n  complete        | norm: 1.0000\n  subject1        | norm: 1.0000\n  subject2        | norm: 1.0000\n  subject3        | norm: 1.0000\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"# Initialize feed service\nprint(\"\\nüéØ Initializing Feed Service...\\n\")\nfeed_service = FeedService(user)\n\n# Generate initial feed\ninitial_feed = feed_service.get_feed(\n    query=None,\n    page=1,\n    page_size=10,\n    apply_mmr=True,\n    mmr_lambda=0.7\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:20:49.497338Z","iopub.execute_input":"2025-11-03T14:20:49.497788Z","iopub.status.idle":"2025-11-03T14:20:55.087983Z","shell.execute_reply.started":"2025-11-03T14:20:49.497752Z","shell.execute_reply":"2025-11-03T14:20:55.086659Z"}},"outputs":[{"name":"stdout","text":"\nüéØ Initializing Feed Service...\n\n\nüìù Generating feed for page 1...\n  - Mode: Recommendation Feed (Profile Only)\n\n1Ô∏è‚É£ Building retrievers for RRF fusion...\n  ‚úì Found 0 results (Semantic Query)\nBuilding complete profile retriever...\n  ‚úì Found 100 results (Complete Profile)\nBuilding Subject 1 retriever...\n  ‚úì Found 100 results (Large Language Models )\nBuilding Subject 2 retriever...\n  ‚úì Found 100 results (Reinforcement Learning)\nBuilding Subject 3 retriever...\n  ‚úì Found 100 results (Economics with Machine learning)\nBuilding Discovery retriever (serendipity)...\n  ‚úì Found 100 results (Discovery)\nBuilding recency retriever...\n  ‚úì Found 0 unique papers (Recency)\n\n2Ô∏è‚É£ Applying RRF fusion...\n  - Fusing: Complete Profile list\n  - Fusing: Subject 1 list\n  - Fusing: Subject 2 list\n  - Fusing: Subject 3 list\n  - Fusing: Discovery list\n  ‚úì RRF fused 450 papers from 5 lists\n\n3Ô∏è‚É£ Preparing for MMR ranking...\n\n4Ô∏è‚É£ Applying MMR diversity filter (top 200 by RRF)...\n  ‚úì MMR selected 50 papers for buffer.\n  ‚úì Filtered out 0 already-interacted items from buffer.\n\n5Ô∏è‚É£ Enriching 10 items with metadata...\n‚úÖ Feed generated with 10 items\n\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"# Display initial feed with detailed metrics\nprint(\"\\n\" + \"=\"*120)\nprint(\"üì∞ INITIAL PERSONALIZED FEED (Top 10)\")\nprint(\"=\"*120)\n\nfor item in initial_feed:\n    print(f\"\\n{item.rank}. [{item.arxiv_id}] {item.title[:80]}...\" if len(item.title) > 80 else f\"\\n{item.rank}. [{item.arxiv_id}] {item.title}\")\n    print(f\"   Authors: {', '.join(item.authors[:3])}{'...' if len(item.authors) > 3 else ''}\")\n    print(f\"   Abstract: {item.abstract[:100]}...\" if len(item.abstract) > 100 else f\"   Abstract: {item.abstract}\")\n    print(f\"   Published: {item.published}\")\n    print(f\"   üìä Scores | RRF: {item.rrf_score:.4f} | Relevance: {item.relevance_score:.4f} | Diversity: {item.diversity_score:.4f} | MMR: {item.mmr_score:.4f}\")\n    print(f\"   üîó {item.abs_url}\")\n\nprint(\"\\n\" + \"=\"*120)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:20:55.089801Z","iopub.execute_input":"2025-11-03T14:20:55.090367Z","iopub.status.idle":"2025-11-03T14:20:55.100744Z","shell.execute_reply.started":"2025-11-03T14:20:55.090341Z","shell.execute_reply":"2025-11-03T14:20:55.099554Z"}},"outputs":[{"name":"stdout","text":"\n========================================================================================================================\nüì∞ INITIAL PERSONALIZED FEED (Top 10)\n========================================================================================================================\n\n1. [2412.07031] Large Language Models: An Applied Econometric Framework\n   Authors: Jens Ludwig, Sendhil Mullainathan, Ashesh Rambachan\n   Abstract: How can we use the novel capacities of large language models (LLMs) in\nempirical research? And how c...\n   Published: 2024-12-09T22:37:48Z\n   üìä Scores | RRF: 0.0325 | Relevance: 0.8569 | Diversity: 0.0000 | MMR: 0.5998\n   üîó http://arxiv.org/abs/2412.07031v2\n\n2. [1810.06339] Deep Reinforcement Learning\n   Authors: Yuxi Li\n   Abstract: We discuss deep reinforcement learning in an overview style. We draw a big\npicture, filled with deta...\n   Published: 2018-10-15T13:20:56Z\n   üìä Scores | RRF: 0.0306 | Relevance: 0.8363 | Diversity: 0.3304 | MMR: 0.7097\n   üîó http://arxiv.org/abs/1810.06339v1\n\n3. [2504.20997] Toward Efficient Exploration by Large Language Model Agents\n   Authors: Dilip Arumugam, Thomas L. Griffiths\n   Abstract: A burgeoning area within reinforcement learning (RL) is the design of\nsequential decision-making age...\n   Published: 2025-04-29T17:59:48Z\n   üìä Scores | RRF: 0.0253 | Relevance: 0.8455 | Diversity: 0.2598 | MMR: 0.6688\n   üîó http://arxiv.org/abs/2504.20997v1\n\n4. [1706.06302] Deep Learning in (and of) Agent-Based Models: A Prospectus\n   Authors: Sander van der Hoog\n   Abstract: A very timely issue for economic agent-based models (ABMs) is their empirical\nestimation. This paper...\n   Published: 2017-06-20T08:08:44Z\n   üìä Scores | RRF: 0.0411 | Relevance: 0.8458 | Diversity: 0.1665 | MMR: 0.6681\n   üîó http://arxiv.org/abs/1706.06302v1\n\n5. [1712.00409] Deep Learning Scaling is Predictable, Empirically\n   Authors: Joel Hestness, Sharan Narang, Newsha Ardalani...\n   Abstract: Deep learning (DL) creates impactful advances following a virtuous recipe:\nmodel architecture search...\n   Published: 2017-12-01T17:13:14Z\n   üìä Scores | RRF: 0.0099 | Relevance: 0.8177 | Diversity: 0.1665 | MMR: 0.6611\n   üîó http://arxiv.org/abs/1712.00409v1\n\n6. [2406.04344] Verbalized Machine Learning: Revisiting Machine Learning with Language\n  Models\n   Authors: Tim Z. Xiao, Robert Bamler, Bernhard Sch√∂lkopf...\n   Abstract: Motivated by the progress made by large language models (LLMs), we introduce\nthe framework of verbal...\n   Published: 2024-06-06T17:59:56Z\n   üìä Scores | RRF: 0.0301 | Relevance: 0.8396 | Diversity: 0.1665 | MMR: 0.6568\n   üîó http://arxiv.org/abs/2406.04344v3\n\n7. [2108.07783] Toward a `Standard Model' of Machine Learning\n   Authors: Zhiting Hu, Eric P. Xing\n   Abstract: Machine learning (ML) is about computational methods that enable machines to\nlearn concepts from exp...\n   Published: 2021-08-17T17:44:38Z\n   üìä Scores | RRF: 0.0139 | Relevance: 0.8308 | Diversity: 0.1665 | MMR: 0.6534\n   üîó http://arxiv.org/abs/2108.07783v2\n\n8. [1903.10075] Machine Learning Methods Economists Should Know About\n   Authors: Susan Athey, Guido Imbens\n   Abstract: We discuss the relevance of the recent Machine Learning (ML) literature for\neconomics and econometri...\n   Published: 2019-03-24T22:58:02Z\n   üìä Scores | RRF: 0.0434 | Relevance: 0.8347 | Diversity: 0.1665 | MMR: 0.6518\n   üîó http://arxiv.org/abs/1903.10075v1\n\n9. [2310.13595] The History and Risks of Reinforcement Learning and Human Feedback\n   Authors: Nathan Lambert, Thomas Krendl Gilbert, Tom Zick\n   Abstract: Reinforcement learning from human feedback (RLHF) has emerged as a powerful\ntechnique to make large ...\n   Published: 2023-10-20T15:45:16Z\n   üìä Scores | RRF: 0.0101 | Relevance: 0.8181 | Diversity: 0.1665 | MMR: 0.6495\n   üîó http://arxiv.org/abs/2310.13595v2\n\n10. [2401.07345] Learning to be Homo Economicus: Can an LLM Learn Preferences from Choice\n   Authors: Jeongbin Kim, Matthew Kovach, Kyu-Min Lee...\n   Abstract: This paper explores the use of Large Language Models (LLMs) as decision aids,\nwith a focus on their ...\n   Published: 2024-01-14T19:05:45Z\n   üìä Scores | RRF: 0.0141 | Relevance: 0.8344 | Diversity: 0.1665 | MMR: 0.6470\n   üîó http://arxiv.org/abs/2401.07345v1\n\n========================================================================================================================\n","output_type":"stream"}],"execution_count":81},{"cell_type":"markdown","source":"## Simulate User Interactions & Update Vectors","metadata":{}},{"cell_type":"code","source":"# Simulate user interactions with decay learning\nprint(\"\\n\" + \"=\"*120)\nprint(\"üîÑ SIMULATING USER INTERACTIONS (with Decay Learning)\")\nprint(\"=\"*120)\n\ninteractions = [\n    (initial_feed[0].arxiv_id, \"LIKE\", datetime.now() - timedelta(days=1)),\n    (initial_feed[1].arxiv_id, \"BOOKMARK\", datetime.now() - timedelta(days=2)),\n    (initial_feed[3].arxiv_id, \"VIEW\", datetime.now() - timedelta(days=0)),\n    (initial_feed[4].arxiv_id, \"DISLIKE\", datetime.now() - timedelta(days=3)),\n    (initial_feed[7].arxiv_id, \"LIKE\", datetime.now() - timedelta(days=0.5)),\n]\n\nprint(\"\\nLogging interactions with decay-weighted updates...\\n\")\nfor arxiv_id, action, timestamp in interactions:\n    feed_service.log_interaction(arxiv_id, action)\n    time.sleep(0.5)  # Brief pause for readability\n\nprint(\"\\nüìä Updated Vector Status (After Interactions):\")\nfor vtype, vec in user.vectors.items():\n    norm = np.linalg.norm(vec)\n    print(f\"  {vtype.value:15s} | norm: {norm:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:21:09.638628Z","iopub.execute_input":"2025-11-03T14:21:09.645733Z","iopub.status.idle":"2025-11-03T14:21:13.015902Z","shell.execute_reply.started":"2025-11-03T14:21:09.645635Z","shell.execute_reply":"2025-11-03T14:21:13.014110Z"}},"outputs":[{"name":"stdout","text":"\n========================================================================================================================\nüîÑ SIMULATING USER INTERACTIONS (with Decay Learning)\n========================================================================================================================\n\nLogging interactions with decay-weighted updates...\n\n‚úì Proportional update for LIKE on 2412.07031 across 3 subjects.\n‚úì Logged LIKE on 2412.07031\n‚úì Proportional update for BOOKMARK on 1810.06339 across 3 subjects.\n‚úì Logged BOOKMARK on 1810.06339\n‚úì Proportional update for VIEW on 1706.06302 across 3 subjects.\n‚úì Logged VIEW on 1706.06302\n‚úì Proportional update for DISLIKE on 1712.00409 across 3 subjects.\n‚úì Logged DISLIKE on 1712.00409\n‚úì Proportional update for LIKE on 1903.10075 across 3 subjects.\n‚úì Logged LIKE on 1903.10075\n\nüìä Updated Vector Status (After Interactions):\n  complete        | norm: 1.0000\n  subject1        | norm: 1.0000\n  subject2        | norm: 1.0000\n  subject3        | norm: 1.0000\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"# Generate feed AFTER learning\nprint(\"\\n\" + \"=\"*120)\nprint(\"üîÑ GENERATING PERSONALIZED FEED (AFTER LEARNING)\")\nprint(\"=\"*120)\n\nlearned_feed = feed_service.get_feed(\n    query=None,\n    page=1,\n    page_size=10,\n    apply_mmr=True,\n    mmr_lambda=0.7\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:21:16.859611Z","iopub.execute_input":"2025-11-03T14:21:16.860068Z","iopub.status.idle":"2025-11-03T14:21:23.446333Z","shell.execute_reply.started":"2025-11-03T14:21:16.860035Z","shell.execute_reply":"2025-11-03T14:21:23.445110Z"}},"outputs":[{"name":"stdout","text":"\n========================================================================================================================\nüîÑ GENERATING PERSONALIZED FEED (AFTER LEARNING)\n========================================================================================================================\n\nüìù Generating feed for page 1...\n  - Mode: Recommendation Feed (Profile Only)\n\n1Ô∏è‚É£ Building retrievers for RRF fusion...\n  ‚úì Found 0 results (Semantic Query)\nBuilding complete profile retriever...\n  ‚úì Found 100 results (Complete Profile)\nBuilding Subject 1 retriever...\n  ‚úì Found 100 results (Large Language Models )\nBuilding Subject 2 retriever...\n  ‚úì Found 100 results (Reinforcement Learning)\nBuilding Subject 3 retriever...\n  ‚úì Found 100 results (Economics with Machine learning)\nBuilding Discovery retriever (serendipity)...\n  ‚úì Found 100 results (Discovery)\nBuilding recency retriever...\n  ‚úì Found 10 unique papers (Recency)\n\n2Ô∏è‚É£ Applying RRF fusion...\n  - Fusing: Complete Profile list\n  - Fusing: Subject 1 list\n  - Fusing: Subject 2 list\n  - Fusing: Subject 3 list\n  - Fusing: Discovery list\n  - Fusing: Recency list\n  ‚úì RRF fused 439 papers from 6 lists\n\n3Ô∏è‚É£ Preparing for MMR ranking...\n\n4Ô∏è‚É£ Applying MMR diversity filter (top 200 by RRF)...\n  ‚úì MMR selected 50 papers for buffer.\n  ‚úì Filtered out 3 already-interacted items from buffer.\n\n5Ô∏è‚É£ Enriching 10 items with metadata...\n‚úÖ Feed generated with 10 items\n\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"# Display learned feed\nprint(\"\\n\" + \"=\"*120)\nprint(\"üì∞ LEARNED PERSONALIZED FEED (Top 10)\")\nprint(\"=\"*120)\n\nfor item in learned_feed:\n    print(f\"\\n{item.rank}. [{item.arxiv_id}] {item.title[:80]}...\" if len(item.title) > 80 else f\"\\n{item.rank}. [{item.arxiv_id}] {item.title}\")\n    print(f\"   Authors: {', '.join(item.authors[:3])}{'...' if len(item.authors) > 3 else ''}\")\n    print(f\"   üìä Scores | RRF: {item.rrf_score:.4f} | Relevance: {item.relevance_score:.4f} | Diversity: {item.diversity_score:.4f} | MMR: {item.mmr_score:.4f}\")\n\nprint(\"\\n\" + \"=\"*120)\nprint(\"‚úÖ Feed generation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:21:23.448115Z","iopub.execute_input":"2025-11-03T14:21:23.448575Z","iopub.status.idle":"2025-11-03T14:21:23.460668Z","shell.execute_reply.started":"2025-11-03T14:21:23.448550Z","shell.execute_reply":"2025-11-03T14:21:23.458833Z"}},"outputs":[{"name":"stdout","text":"\n========================================================================================================================\nüì∞ LEARNED PERSONALIZED FEED (Top 10)\n========================================================================================================================\n\n1. [1706.06302] Deep Learning in (and of) Agent-Based Models: A Prospectus\n   Authors: Sander van der Hoog\n   üìä Scores | RRF: 0.0464 | Relevance: 0.8592 | Diversity: 0.1634 | MMR: 0.6690\n\n2. [2504.20997] Toward Efficient Exploration by Large Language Model Agents\n   Authors: Dilip Arumugam, Thomas L. Griffiths\n   üìä Scores | RRF: 0.0149 | Relevance: 0.8355 | Diversity: 0.1634 | MMR: 0.6618\n\n3. [2108.07783] Toward a `Standard Model' of Machine Learning\n   Authors: Zhiting Hu, Eric P. Xing\n   üìä Scores | RRF: 0.0297 | Relevance: 0.8299 | Diversity: 0.1634 | MMR: 0.6531\n\n4. [2406.04344] Verbalized Machine Learning: Revisiting Machine Learning with Language\n  Models\n   Authors: Tim Z. Xiao, Robert Bamler, Bernhard Sch√∂lkopf...\n   üìä Scores | RRF: 0.0254 | Relevance: 0.8338 | Diversity: 0.1634 | MMR: 0.6527\n\n5. [2401.07345] Learning to be Homo Economicus: Can an LLM Learn Preferences from Choice\n   Authors: Jeongbin Kim, Matthew Kovach, Kyu-Min Lee...\n   üìä Scores | RRF: 0.0152 | Relevance: 0.8396 | Diversity: 0.1634 | MMR: 0.6507\n\n6. [1812.09747] Enhancing Discrete Choice Models with Representation Learning\n   Authors: Brian Sifringer, Virginie Lurkin, Alexandre Alahi\n   üìä Scores | RRF: 0.0182 | Relevance: 0.8083 | Diversity: 0.1634 | MMR: 0.6415\n\n7. [1811.0974] Unknown Title\n   Authors: \n   üìä Scores | RRF: 0.0116 | Relevance: 0.8209 | Diversity: 0.1634 | MMR: 0.6410\n\n8. [2411.01271] Interacting Large Language Model Agents. Interpretable Models and Social\n  Learn...\n   Authors: Adit Jain, Vikram Krishnamurthy\n   üìä Scores | RRF: 0.0119 | Relevance: 0.8214 | Diversity: 0.1634 | MMR: 0.6408\n\n9. [2110.04442] A Primer on Deep Learning for Causal Inference\n   Authors: Bernard Koch, Tim Sainburg, Pablo Geraldo...\n   üìä Scores | RRF: 0.0190 | Relevance: 0.8131 | Diversity: 0.1634 | MMR: 0.6404\n\n10. [1612.00367] Large-scale Validation of Counterfactual Learning Methods: A Test-Bed\n   Authors: Damien Lefortier, Adith Swaminathan, Xiaotao Gu...\n   üìä Scores | RRF: 0.0108 | Relevance: 0.7857 | Diversity: 0.1634 | MMR: 0.6391\n\n========================================================================================================================\n‚úÖ Feed generation complete!\n","output_type":"stream"}],"execution_count":84},{"cell_type":"markdown","source":"## Analysis: How RRF Enhanced Recommendations","metadata":{}},{"cell_type":"code","source":"# Compare initial vs learned feeds\nprint(\"\\n\" + \"=\"*120)\nprint(\"üìä COMPARISON: Initial vs Learned Feeds\")\nprint(\"=\"*120)\n\ninitial_ids = set(item.arxiv_id for item in initial_feed)\nlearned_ids = set(item.arxiv_id for item in learned_feed)\n\ncommon = initial_ids & learned_ids\nnew_items = learned_ids - initial_ids\ndropped = initial_ids - learned_ids\n\nprint(f\"\\nüìà Feed Statistics:\")\nprint(f\"  Initial feed: {len(initial_ids)} items\")\nprint(f\"  Learned feed: {len(learned_ids)} items\")\nprint(f\"  Items in common: {len(common)}\")\nprint(f\"  New items after learning: {len(new_items)}\")\nprint(f\"  Items dropped: {len(dropped)}\")\n\nprint(f\"\\nüîÑ How RRF Fusion Improved Ranking:\")\nprint(f\"  RRF combines 4 independent ranking signals:\")\nprint(f\"    1. Semantic Query: Personalized text query + user profile\")\nprint(f\"    2. Complete Profile: Pure aggregated user interests\")\nprint(f\"    3. Subject-Focused: Most relevant subject area embedding\")\nprint(f\"    4. Recency: Recently published papers\")\nprint(f\"\\n  Result: More robust, diverse, and well-ranked recommendations!\")\nprint(f\"\\nüí° Decay Learning Updates:\")\nprint(f\"  - LIKE interactions: +1.0 weight (decays over time)\")\nprint(f\"  - BOOKMARK interactions: +0.8 weight\")\nprint(f\"  - VIEW interactions: +0.3 weight\")\nprint(f\"  - DISLIKE interactions: -0.5 weight\")\nprint(f\"\\n‚úÖ All vectors updated with ALPHA={ALPHA} and DECAY_RATE={DECAY_RATE}\")\nprint(\"\\n\" + \"=\"*120)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:21:35.666031Z","iopub.execute_input":"2025-11-03T14:21:35.667651Z","iopub.status.idle":"2025-11-03T14:21:35.678689Z","shell.execute_reply.started":"2025-11-03T14:21:35.667610Z","shell.execute_reply":"2025-11-03T14:21:35.677511Z"}},"outputs":[{"name":"stdout","text":"\n========================================================================================================================\nüìä COMPARISON: Initial vs Learned Feeds\n========================================================================================================================\n\nüìà Feed Statistics:\n  Initial feed: 10 items\n  Learned feed: 10 items\n  Items in common: 5\n  New items after learning: 5\n  Items dropped: 5\n\nüîÑ How RRF Fusion Improved Ranking:\n  RRF combines 4 independent ranking signals:\n    1. Semantic Query: Personalized text query + user profile\n    2. Complete Profile: Pure aggregated user interests\n    3. Subject-Focused: Most relevant subject area embedding\n    4. Recency: Recently published papers\n\n  Result: More robust, diverse, and well-ranked recommendations!\n\nüí° Decay Learning Updates:\n  - LIKE interactions: +1.0 weight (decays over time)\n  - BOOKMARK interactions: +0.8 weight\n  - VIEW interactions: +0.3 weight\n  - DISLIKE interactions: -0.5 weight\n\n‚úÖ All vectors updated with ALPHA=0.1 and DECAY_RATE=0.1\n\n========================================================================================================================\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}